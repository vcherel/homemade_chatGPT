{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561c3744",
   "metadata": {},
   "source": [
    "# Homemade chat GPT (from IMDb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8fa8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 19:23:51.253018: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-15 19:23:51.258177: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-15 19:23:51.315094: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-15 19:23:52.509362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home/valentin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the modules\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, download\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from utils import clean_utterance\n",
    "\n",
    "download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88573de",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c308741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sample: ['pos', 'For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.']\n",
      "Last sample: ['neg', 'Not that I dislike childrens movies, but this was a tearjerker with few redeeming qualities. M.J. Fox was the perfect voice for Stuart and the rest of the talent was wasted. Hugh Laurie can be amazingly funny, but is not given the chance in this movie. It´s sugar-coated sugar and would hardly appeal to anyone over 7 years of age. See Toy Story, Monsters Inc. or Shrek instead. 3/10']\n",
      "tokenized text[i] = ['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', 'the', 'skipper', 'hale', 'jr.', 'as', 'a', 'police', 'sgt', '.']\n",
      "tokenized text[i] = ['bizarre', 'horror', 'movie', 'filled', 'with', 'famous', 'faces', 'but', 'stolen', 'by', 'cristina', 'raines', 'later', 'of', 'tv', \"'s\", 'flamingo', 'road', 'as', 'a', 'pretty', 'but', 'somewhat', 'unstable', 'model', 'with', 'a', 'gummy', 'smile', 'who', 'is', 'slated', 'to', 'pay', 'for', 'her', 'attempted', 'suicides', 'by', 'guarding', 'the', 'gateway', 'to', 'hell', '!', 'the', 'scenes', 'with', 'raines', 'modeling', 'are', 'very', 'well', 'captured', ',', 'the', 'mood', 'music', 'is', 'perfect', ',', 'deborah', 'raffin', 'is', 'charming', 'as', 'cristina', \"'s\", 'pal', ',', 'but', 'when', 'raines', 'moves', 'into', 'a', 'creepy', 'brooklyn', 'heights', 'brownstone', 'inhabited', 'by', 'a', 'blind', 'priest', 'on', 'the', 'top', 'floor', ',', 'things', 'really', 'start', 'cooking', '.', 'the', 'neighbors', ',', 'including', 'a', 'fantastically', 'wicked', 'burgess', 'meredith', 'and', 'kinky', 'couple', 'sylvia', 'miles', '&', 'beverly', \"d'angelo\", ',', 'are', 'a', 'diabolical', 'lot', ',', 'and', 'eli', 'wallach', 'is', 'great', 'fun', 'as', 'a', 'wily', 'police', 'detective', '.', 'the', 'movie', 'is', 'nearly', 'a', 'cross-pollination', 'of', 'rosemary', \"'s\", 'baby', 'and', 'the', 'exorcist', 'but', 'what', 'a', 'combination', '!', 'based', 'on', 'the', 'best-seller', 'by', 'jeffrey', 'konvitz', ',', 'the', 'sentinel', 'is', 'entertainingly', 'spooky', ',', 'full', 'of', 'shocks', 'brought', 'off', 'well', 'by', 'director', 'michael', 'winner', ',', 'who', 'mounts', 'a', 'thoughtfully', 'downbeat', 'ending', 'with', 'skill', '.', '1/2', 'from']\n",
      "tokenized text[i] = ['a', 'solid', ',', 'if', 'unremarkable', 'film', '.', 'matthau', ',', 'as', 'einstein', ',', 'was', 'wonderful', '.', 'my', 'favorite', 'part', ',', 'and', 'the', 'only', 'thing', 'that', 'would', 'make', 'me', 'go', 'out', 'of', 'my', 'way', 'to', 'see', 'this', 'again', ',', 'was', 'the', 'wonderful', 'scene', 'with', 'the', 'physicists', 'playing', 'badmitton', ',', 'i', 'loved', 'the', 'sweaters', 'and', 'the', 'conversation', 'while', 'they', 'waited', 'for', 'robbins', 'to', 'retrieve', 'the', 'birdie', '.']\n",
      "tokenized text[i] = ['it', \"'s\", 'a', 'strange', 'feeling', 'to', 'sit', 'alone', 'in', 'a', 'theater', 'occupied', 'by', 'parents', 'and', 'their', 'rollicking', 'kids', '.', 'i', 'felt', 'like', 'instead', 'of', 'a', 'movie', 'ticket', ',', 'i', 'should', 'have', 'been', 'given', 'a', 'nambla', 'membership.', 'based', 'upon', 'thomas', 'rockwell', \"'s\", 'respected', 'book', ',', 'how', 'to', 'eat', 'fried', 'worms', 'starts', 'like', 'any', 'children', \"'s\", 'story', ':', 'moving', 'to', 'a', 'new', 'town', '.', 'the', 'new', 'kid', ',', 'fifth', 'grader', 'billy', 'forrester', 'was', 'once', 'popular', ',', 'but', 'has', 'to', 'start', 'anew', '.', 'making', 'friends', 'is', 'never', 'easy', ',', 'especially', 'when', 'the', 'only', 'prospect', 'is', 'poindexter', 'adam', '.', 'or', 'erica', ',', 'who', 'at', '4', '1/2', 'feet', ',', 'is', 'a', 'giant.', 'further', 'complicating', 'things', 'is', 'joe', 'the', 'bully', '.', 'his', 'freckled', 'face', 'and', 'sleeveless', 'shirts', 'are', 'daunting', '.', 'he', 'antagonizes', 'kids', 'with', 'the', 'death', 'ring', ':', 'a', 'crackerjack', 'ring', 'that', 'is', 'rumored', 'to', 'kill', 'you', 'if', 'you', \"'re\", 'punched', 'with', 'it', '.', 'but', 'not', 'immediately', '.', 'no', ',', 'the', 'death', 'ring', 'unleashes', 'a', 'poison', 'that', 'kills', 'you', 'in', 'the', 'eight', 'grade.', 'joe', 'and', 'his', 'axis', 'of', 'evil', 'welcome', 'billy', 'by', 'smuggling', 'a', 'handful', 'of', 'slimy', 'worms', 'into', 'his', 'thermos', '.', 'once', 'discovered', ',', 'billy', 'plays', 'it', 'cool', ',', 'swearing', 'that', 'he', 'eats', 'worms', 'all', 'the', 'time', '.', 'then', 'he', 'throws', 'them', 'at', 'joe', \"'s\", 'face', '.', 'ewww', '!', 'to', 'win', 'them', 'over', ',', 'billy', 'reluctantly', 'bets', 'that', 'he', 'can', 'eat', '10', 'worms', '.', 'fried', ',', 'boiled', ',', 'marinated', 'in', 'hot', 'sauce', ',', 'squashed', 'and', 'spread', 'on', 'a', 'peanut', 'butter', 'sandwich', '.', 'each', 'meal', 'is', 'dubbed', 'an', 'exotic', 'name', 'like', 'the', 'radioactive', 'slime', 'delight', ',', 'in', 'which', 'the', 'kids', 'finally', 'live', 'out', 'their', 'dream', 'of', 'microwaving', 'a', 'living', 'organism.', 'if', 'you', \"'ve\", 'ever', 'met', 'me', ',', 'you', \"'ll\", 'know', 'that', 'i', 'have', 'an', 'uncontrollably', 'hearty', 'laugh', '.', 'i', 'felt', 'like', 'a', 'creep', 'erupting', 'at', 'a', 'toddler', 'whining', 'that', 'his', 'dilly', 'dick', 'hurts', '.', 'but', 'fried', 'worms', 'is', 'wonderfully', 'disgusting', '.', 'like', 'a', 'g-rated', 'farrelly', 'brothers', 'film', ',', 'it', 'is', 'both', 'vomitous', 'and', 'delightful.', 'writer/director', 'bob', 'dolman', 'is', 'also', 'a', 'savvy', 'storyteller', '.', 'to', 'raise', 'the', 'stakes', 'the', 'worms', 'must', 'be', 'consumed', 'by', '7', 'pm', '.', 'in', 'addition', 'billy', 'holds', 'a', 'dark', 'secret', ':', 'he', 'has', 'an', 'ultra-sensitive', 'stomach.', 'dolman', 'also', 'has', 'a', 'keen', 'sense', 'of', 'perspective', '.', 'with', 'such', 'accuracy', ',', 'he', 'draws', 'on', 'children', \"'s\", 'insecurities', 'and', 'tendency', 'to', 'exaggerate', 'mundane', 'dilemmas.', 'if', 'you', 'were', 'to', 'hyperbolize', 'this', 'movie', 'the', 'way', 'kids', 'do', 'their', 'quandaries', ',', 'you', 'will', 'see', 'that', 'it', 'is', 'essentially', 'about', 'war', '.', 'freedom-fighter', 'and', 'freedom-hater', 'use', 'pubescent', 'boys', 'as', 'pawns', 'in', 'proxy', 'wars', ',', 'only', 'to', 'learn', 'a', 'valuable', 'lesson', 'in', 'unity', '.', 'international', 'leaders', 'can', 'learn', 'a', 'thing', 'or', 'two', 'about', 'global', 'peacekeeping', 'from', 'fried', 'worms.', 'at', 'the', 'end', 'of', 'the', 'film', ',', 'i', 'was', 'comforted', 'when', 'two', 'chaperoning', 'mothers', 'behind', 'me', ',', 'looked', 'at', 'each', 'other', 'with', 'befuddlement', 'and', 'agreed', ',', 'that', 'was', 'a', 'great', 'movie', '.', 'great', ',', 'now', 'i', 'wo', \"n't\", 'have', 'to', 'register', 'myself', 'in', 'any', 'lawful', 'databases', '.']\n",
      "tokenized text[i] = ['you', 'probably', 'all', 'already', 'know', 'this', 'by', 'now', ',', 'but', '5', 'additional', 'episodes', 'never', 'aired', 'can', 'be', 'viewed', 'on', 'abc.com', 'i', \"'ve\", 'watched', 'a', 'lot', 'of', 'television', 'over', 'the', 'years', 'and', 'this', 'is', 'possibly', 'my', 'favorite', 'show', ',', 'ever', '.', 'it', \"'s\", 'a', 'crime', 'that', 'this', 'beautifully', 'written', 'and', 'acted', 'show', 'was', 'canceled', '.', 'the', 'actors', 'that', 'played', 'laura', ',', 'whit', ',', 'carlos', ',', 'mae', ',', 'damian', ',', 'anya', 'and', 'omg', ',', 'steven', 'caseman', 'are', 'all', 'incredible', 'and', 'so', 'natural', 'in', 'those', 'roles', '.', 'even', 'the', 'kids', 'are', 'great', '.', 'wonderful', 'show', '.', 'so', 'sad', 'that', 'it', \"'s\", 'gone', '.', 'of', 'course', 'i', 'wonder', 'about', 'the', 'reasons', 'it', 'was', 'canceled', '.', 'there', 'is', 'no', 'way', 'i', \"'ll\", 'let', 'myself', 'believe', 'that', 'ms.', 'moynahan', \"'s\", 'pregnancy', 'had', 'anything', 'to', 'do', 'with', 'it', '.', 'it', 'was', 'in', 'the', 'perfect', 'time', 'slot', 'in', 'this', 'market', '.', 'i', \"'ve\", 'watched', 'all', 'the', 'episodes', 'again', 'on', 'abc.com', 'i', 'hope', 'they', 'all', 'come', 'out', 'on', 'dvd', 'some', 'day', '.', 'thanks', 'for', 'reading', '.']\n",
      "tokenized text[i] = ['i', 'saw', 'the', 'movie', 'with', 'two', 'grown', 'children', '.', 'although', 'it', 'was', 'not', 'as', 'clever', 'as', 'shrek', ',', 'i', 'thought', 'it', 'was', 'rather', 'good', '.', 'in', 'a', 'movie', 'theatre', 'surrounded', 'by', 'children', 'who', 'were', 'on', 'spring', 'break', ',', 'there', 'was', 'not', 'a', 'sound', 'so', 'i', 'know', 'the', 'children', 'all', 'liked', 'it', '.', 'there', 'parents', 'also', 'seemed', 'engaged', '.', 'the', 'death', 'and', 'apparent', 'death', 'of', 'characters', 'brought', 'about', 'the', 'appropriate', 'gasps', 'and', 'comments', '.', 'hopefully', 'people', 'realize', 'this', 'movie', 'was', 'made', 'for', 'kids', '.', 'as', 'such', ',', 'it', 'was', 'successful', 'although', 'i', 'liked', 'it', 'too', '.', 'personally', 'i', 'liked', 'the', 'scrat', '!', '!']\n",
      "tokenized text[i] = ['you', \"'re\", 'using', 'the', 'imdb.', 'you', \"'ve\", 'given', 'some', 'hefty', 'votes', 'to', 'some', 'of', 'your', 'favourite', 'films.', 'it', \"'s\", 'something', 'you', 'enjoy', 'doing.', 'and', 'it', \"'s\", 'all', 'because', 'of', 'this', '.', 'fifty', 'seconds', '.', 'one', 'world', 'ends', ',', 'another', 'begins.', 'how', 'can', 'it', 'not', 'be', 'given', 'a', 'ten', '?', 'i', 'wonder', 'at', 'those', 'who', 'give', 'this', 'a', 'seven', 'or', 'an', 'eight', '...', 'exactly', 'how', 'could', 'the', 'first', 'film', 'ever', 'made', 'be', 'better', '?', 'for', 'the', 'record', ',', 'the', 'long', ',', 'still', 'opening', 'shot', 'is', 'great', 'showmanship', ',', 'a', 'superb', 'innovation', ',', 'perfectly', 'suited', 'to', 'the', 'situation', '.', 'and', 'the', 'dog', 'on', 'the', 'bike', 'is', 'a', 'lovely', 'touch', '.', 'all', 'this', 'within', 'fifty', 'seconds.', 'the', 'word', 'genius', 'is', 'often', 'overused.', 'this', 'is', 'genius', '.']\n",
      "tokenized text[i] = ['this', 'was', 'a', 'good', 'film', 'with', 'a', 'powerful', 'message', 'of', 'love', 'and', 'redemption', '.', 'i', 'loved', 'the', 'transformation', 'of', 'the', 'brother', 'and', 'the', 'repercussions', 'of', 'the', 'horrible', 'disease', 'on', 'the', 'family', '.', 'well-acted', 'and', 'well-directed', '.', 'if', 'there', 'were', 'any', 'flaws', ',', 'i', \"'d\", 'have', 'to', 'say', 'that', 'the', 'story', 'showed', 'the', 'typical', 'suburban', 'family', 'and', 'their', 'difficulties', 'again', '.', 'what', 'about', 'all', 'people', 'of', 'all', 'cultural', 'backgrounds', '?', 'i', 'would', 'love', 'to', 'see', 'a', 'movie', 'where', 'all', 'of', 'these', 'cultures', 'are', 'shown', 'like', 'in', 'real', 'life', '.', 'nevertheless', ',', 'the', 'film', 'soared', 'in', 'terms', 'of', 'its', 'values', 'and', 'its', 'understanding', 'of', 'the', 'how', 'a', 'disease', 'can', 'bring', 'someone', 'closer', 'to', 'his', 'or', 'her', 'maker', '.', 'loved', 'the', 'film', 'and', 'it', 'brought', 'tears', 'to', 'my', 'eyes']\n",
      "tokenized text[i] = ['made', 'after', 'quartet', 'was', ',', 'trio', 'continued', 'the', 'quality', 'of', 'the', 'earlier', 'film', 'versions', 'of', 'the', 'short', 'stories', 'by', 'maugham', '.', 'here', 'the', 'three', 'stories', 'are', 'the', 'verger', ',', 'mr.', 'know-it-all', ',', 'and', 'sanitorium', '.', 'the', 'first', 'two', 'are', 'comic', 'the', 'verger', 'is', 'like', 'a', 'prolonged', 'joke', ',', 'but', 'one', 'with', 'a', 'good', 'pay-off', ',', 'and', 'the', 'last', 'more', 'serious', 'as', 'health', 'issues', 'are', 'involved', '.', 'again', 'the', 'author', 'introduces', 'the', 'film', 'and', 'the', 'stories.', 'james', 'hayter', ',', 'soon', 'to', 'have', 'his', 'signature', 'role', 'as', 'samuel', 'pickwick', ',', 'is', 'the', 'hero', 'in', 'the', 'verger', '.', 'he', 'holds', 'this', 'small', 'custodial-type', 'job', 'in', 'a', 'church', ',', 'but', 'the', 'new', 'vicar', 'michael', 'hordern', 'is', 'an', 'intellectual', 'snob', '.', 'when', 'he', 'hears', 'hayter', 'has', 'no', 'schooling', 'he', 'fires', 'him', '.', 'hayter', 'has', 'saved', 'some', 'money', ',', 'so', 'he', 'tells', 'his', 'wife', 'kathleen', 'harrison', 'he', 'fancies', 'buying', 'a', 'small', 'news', 'and', 'tobacco', 'shop', '.', 'he', 'has', 'a', 'good', 'eye', ',', 'and', 'his', 'store', 'thrives', '.', 'soon', 'he', 'has', 'a', 'whole', 'chain', 'of', 'stores', '.', 'when', 'his', 'grandchild', 'is', 'christened', 'by', 'hordern', ',', 'the', 'latter', 'is', 'amazed', 'to', 'see', 'how', 'prosperous', 'his', 'ex-verger', '.', 'the', 'payoff', 'is', 'when', 'bank', 'manager', 'felix', 'aylmer', 'meets', 'with', 'hayter', 'about', 'diversifying', 'his', 'investments', '.', 'i', \"'ll\", 'leave', 'it', 'to', 'you', 'to', 'hear', 'the', 'unintentional', 'but', 'ironic', 'coda', 'of', 'the', 'meeting.', 'according', 'to', 'maugham', 'he', 'met', 'a', 'man', 'like', 'max', 'kelada', 'nigel', 'patrick', 'on', 'a', 'cruise', '.', 'in', 'mr.', 'know-it-all', 'kelada', 'is', 'a', 'splashy', ',', 'friendly', ',', 'and', 'slightly', 'overbearing', 'type', 'from', 'the', 'middle', 'east', 'who', 'is', 'on', 'a', 'business', 'trip', 'regarding', 'jewelry', 'by', 'steamship', '.', 'his', 'state-room', 'mate', 'is', 'mr.', 'grey', 'the', 'ever', 'quiet', 'and', 'proper', 'wilfred', 'hyde-white', 'who', 'is', 'somewhat', ',', 'silently', 'disapproving', 'of', 'max', '.', 'max', 'likes', 'to', 'enliven', 'things', ',', 'and', 'soon', 'is', 'heavily', 'involved', 'in', 'the', 'ship', \"'s\", 'entertainment', '.', 'at', 'this', 'point', 'the', 'story', 'actually', 'resembles', 'part', 'of', 'the', 'plot', 'of', 'the', 'non-maugham', 'story', 'and', 'film', 'china', 'seas', '1935', ',', 'as', 'max', 'makes', 'a', 'bet', 'that', 'he', 'can', 'tell', 'a', 'real', 'piece', 'of', 'jewelry', 'from', 'a', 'fake', 'after', 'insisting', 'that', 'a', 'piece', 'of', 'jewelry', 'he', 'spotted', 'is', 'real', '.', 'i', 'wo', \"n't\", 'describe', 'the', 'way', 'max', 'rises', 'to', 'the', 'occasion.', 'sanitorium', 'is', 'the', 'longest', 'segment', '.', 'roland', 'culver', 'plays', 'ashenden', 'the', 'fictional', 'alter-ego', 'of', 'maugham', 'a', 'writer', 'and', 'one', 'time', 'spy', 'as', 'in', 'hitchcock', \"'s\", 'the', 'secret', 'agent', '.', 'here', 'he', 'has', 'to', 'use', 'a', 'sanitorium', 'for', 'a', 'couple', 'of', 'months', 'for', 'his', 'health', '.', 'he', 'finds', 'a', 'remarkable', 'crew', 'of', 'people', ',', 'including', 'jean', 'simmons', 'as', 'a', 'frail', 'but', 'beautiful', 'young', 'woman', ',', 'finlay', 'currie', 'as', 'an', 'irascible', 'scotsman', ',', 'john', 'laurie', 'as', 'a', 'second', 'irascible', 'scotsman', 'who', 'is', 'at', 'war', 'with', 'currie', ',', 'raymond', 'huntley', 'as', 'a', 'quiet', 'patient', 'who', 'only', 'shows', 'his', 'internal', 'anger', 'at', 'his', 'situation', 'when', 'his', 'wife', 'shows', 'up', ',', 'and', 'michael', 'rennie', 'as', 'a', 'young', 'man', 'who', 'has', 'a', 'serious', 'life', 'threatening', 'illness', '.', 'culver', 'watches', 'as', 'three', 'stories', 'among', 'these', 'characters', 'play', 'out', 'to', 'their', 'conclusions', '.', 'the', 'last', ',', 'dealing', 'with', 'simmons', 'and', 'rennie', ',', 'is', 'ironic', 'but', 'deeply', 'moving.', 'it', 'was', 'a', 'dandy', 'follow-up', 'to', 'the', 'earlier', 'quartet', ',', 'and', 'well', 'worth', 'watching', '.']\n",
      "tokenized text[i] = ['for', 'a', 'mature', 'man', ',', 'to', 'admit', 'that', 'he', 'shed', 'a', 'tear', 'over', 'this', 'film', 'is', 'a', 'mature', 'response', ',', 'to', 'a', 'mature', 'film.', 'if', 'one', 'need', 'admit', 'more', 'then', 'perhaps', 'one', 'could', 'say', 'that', ',', 'life', 'can', 'never', 'be', 'the', 'same', ',', 'after', 'viewing', 'such', 'advent', 'for', 'it', 'has', 'moved', 'us', 'to', 'the', 'next', 'level.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load the dataset and limit the number of samples\n",
    "with open('./IMDb_dataset.json', 'rt') as f:\n",
    "    imdb_data = json.load(f)\n",
    "\n",
    "\n",
    "# We create a list of tokenized positive and negative texts\n",
    "print(\"Starting tokenization for positive texts\")\n",
    "pos_texts = [clean_utterance(word_tokenize(x[1])) for x in imdb_data if x[0] == 'pos']\n",
    "\n",
    "print(\"Starting tokenization for negative texts\")\n",
    "neg_texts = [clean_utterance(word_tokenize(x[1])) for x in imdb_data if x[0] == 'neg']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3529c3",
   "metadata": {},
   "source": [
    "### Prepare the vocabulary on which we will operate\n",
    "\n",
    "This is the standard stuff that you should already be acquainted with. \n",
    "\n",
    "We provide a function to get token counts, i.e., list all tokens present in list of texts along with their number of occurrences. The function returns a dictionary where the keys are the tokens and the values the number of occurrences in the dataset.\n",
    "\n",
    "Up to you to select your vocabulary and create the corresponding token_to_id and id_to_token mappings. Because, after selection of the vocabulary, some of the tokens will not be part of the vocabulary, we need to insert the special token '<unk>'. \n",
    "    \n",
    "1. Create a vocabulary with tokens that appear at least 30 or 20 times in the data, plus the <unk> token. The vocabulary is typically represented as a dictionary that maps tokens (strings) to integers so you can call vocab['real'] to get the integer id of the token 'real'. Also create the inverse mapping from id to token as a \n",
    "list (not necessary but very useful for debugging and pretty printing).\n",
    "    \n",
    "2. Encode pos_texts and neg_texts as list of integers with your vocabulary\n",
    "    \n",
    "    \n",
    "We provide in the next cell two useful functions to encode/decode a sequence: list of strings to list of integers for the former, and conversely for the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "538bf5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of tokens in dataset = 77041\n",
      "most frequent tokens:\n",
      "   the                   172318\n",
      "   ,                     144077\n",
      "   .                     117678\n",
      "   and                   89398\n",
      "   a                     83300\n",
      "   of                    76630\n",
      "   to                    66455\n",
      "   is                    58467\n",
      "   in                    49797\n",
      "   it                    47350\n",
      "   i                     40267\n",
      "   that                  35526\n",
      "   this                  34881\n",
      "   's                    32132\n",
      "   as                    26253\n",
      "   with                  23197\n",
      "   was                   22685\n",
      "   for                   22303\n",
      "   but                   20731\n",
      "   film                  20284\n",
      "\n",
      "least frequent tokens:\n",
      "   vulgarities           1\n",
      "   rêves                 1\n",
      "   objectifier           1\n",
      "   disaster.one          1\n",
      "   ketty                 1\n",
      "   konstadinou           1\n",
      "   kavogianni            1\n",
      "   'guilty               1\n",
      "   laughing.every        1\n",
      "   heart.my              1\n",
      "   vassilis              1\n",
      "   haralambopoulos       1\n",
      "   athinodoros           1\n",
      "   prousalis             1\n",
      "   nikolaidis            1\n",
      "   tv.in                 1\n",
      "   ant1                  1\n",
      "   zones.we              1\n",
      "   imy                   1\n",
      "   jayden                1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Scan the list of texts and return a list of the tokens therein along with their\n",
    "# number of occurrences, sorted in descending order.\n",
    "#\n",
    "def get_token_counts(data: list) -> dict:\n",
    "    '''\n",
    "    Create vocabulary from a bunch of (tokenized) texts. \n",
    "    \n",
    "    Returns:\n",
    "        - token count (dict)\n",
    "    '''\n",
    "\n",
    "    tokcnt = {}    \n",
    "    \n",
    "    for utterance in data:\n",
    "        for token in utterance:\n",
    "            tokcnt[token] = 1 if token not in tokcnt else tokcnt[token] + 1\n",
    "\n",
    "    return dict(sorted(tokcnt.items(), key=lambda x: x[1], reverse = True))\n",
    "\n",
    "\n",
    "count = get_token_counts(pos_texts)\n",
    "\n",
    "#\n",
    "# Pretty print a number of things\n",
    "#\n",
    "print('total number of tokens in dataset =', len(count))\n",
    "print('most frequent tokens:')\n",
    "for x in list(count.keys())[:20]:\n",
    "    print(f\"   {x:20}  {count[x]}\")\n",
    "print('\\nleast frequent tokens:')\n",
    "for x in list(count.keys())[-20:]:\n",
    "    print(f\"   {x:20}  {count[x]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "123c4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create a vocabulary with tokens that appear at least MINOCC times in the data, plus the <unk> token \n",
    "# with id 0.\n",
    "#\n",
    "# The vocabulary is typically represented as a dictionary that maps tokens (strings) to integers so you \n",
    "# can call vocab['real'] to get the integer id of the token 'real'. Also create the inverse mapping \n",
    "# from id to token as a string list (not necessary but very useful for debugging and pretty printing).\n",
    "#\n",
    "\n",
    "MINOCC = 30 # you can also play with smaller number and see what happens to your generator\n",
    "\n",
    "int_to_word = []\n",
    "vocab = {}\n",
    "\n",
    "int_id = 1\n",
    "vocab['<unk>'] = 0\n",
    "int_to_word.append('<unk>')\n",
    "for token in count:\n",
    "    if count[token] >= MINOCC:\n",
    "        vocab[token] = int_id\n",
    "        int_to_word.append(token)\n",
    "        int_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd059ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 2. Encode pos_texts and neg_texts as list of integers with your vocabulary\n",
    "#\n",
    "# We strongly suggest to write an encode_sequence function that takes as input a list of \n",
    "# tokens (strings) and outputs the corresponding list of integers given the vocabulary. \n",
    "# Having a decode_sequence function that does the inverse operation is also very practical.\n",
    "#\n",
    "# Encoding the first positive comment should give something like (showing comment as a list \n",
    "# of tokens, then encoded as a list of ids):\n",
    "#\n",
    "# ['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny', '!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer', '.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'watch', 'for', 'alan', 'the', 'skipper', 'hale', 'jr.', 'as', 'a', 'police', 'sgt', '.']\n",
    "# [18, 5, 21, 12, 231, 90, 1144, 46, 268, 26, 5, 166, 6, 648, 4172, 4427, 18, 13, 1031, 3, 918, 5, 21, 125, 772, 0, 8, 190, 182, 33, 5351, 0, 8, 5, 136, 0, 3, 1, 0, 114, 8, 36, 1744, 2434, 3, 117, 18, 1283, 1, 0, 0, 1837, 15, 5, 531, 6210, 3]\n",
    "#\n",
    "# In the end, pos_texts should be converted to a list of list of integers.\n",
    "#\n",
    "\n",
    "def encode_sequence(tokens: list[str], vocab: dict) -> list[int]:\n",
    "    '''\n",
    "    Encode a list of tokens into a list of integers using a vocabulary.\n",
    "    '''\n",
    "    \n",
    "    return [vocab[token] if token in vocab else 0 for token in tokens]\n",
    "\n",
    "\n",
    "encoded_pos_texts = [encode_sequence(x, vocab) for x in pos_texts]\n",
    "encoded_neg_texts = [encode_sequence(x, vocab) for x in neg_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7589d4c",
   "metadata": {},
   "source": [
    "## Train a basic RNN-LM model\n",
    "\n",
    "We will guide you through the following steps:\n",
    "\n",
    "1. Prepare the training data for LM with a large number of pairs (history, next_token)\n",
    "2. Define the model as a tensorflow.keras model\n",
    "3. Train the model and play with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571bd9c6",
   "metadata": {},
   "source": [
    "#### Prepare training data for LM\n",
    "\n",
    "We first need to prepare training data for our simplified LM model. Here, training data consists of fixed-length sequences with the corresponding label, i.e., the token that follows. For a 6-gram model, we thus have (all of these should be encoded as integers of course)\n",
    "\n",
    "```\n",
    "['for', 'a', 'movie', 'that', 'gets']  >>  no\n",
    "['that', 'gets', 'no', 'respect', 'there']  >>  sure\n",
    "['respect', 'there', 'sure', 'are', 'a']  >>  lot\n",
    "['are', 'a', 'lot', 'of', 'memorable']  >>  quotes\n",
    "``` \n",
    "\n",
    "We will design a closed vocabulary LM (i.e., no possibility of assigning probability to the <unk> token or having the <unk> token in the history), we discard sequences (history and label) where <unk> appears. As training data are documents rather than sentences, we also avoid sequences with end of sentence punctuation marks in the history (here, only the period is considered).\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Create two arrays X and Y containing the training data, where\n",
    "- X[i]: list[int] ==> history represented as as sequence of token ids for the i'th training sample \n",
    "- Y[i]: int       ==> token id to predict following X[i]\n",
    "    \n",
    "To generate the training samples, scan the input sequences and retain all couples (history, prediction) that you can make and where the <unk> token does not appear (neither in the history, nor in the prediction). The end-of-sentence punctuation (.) should also not appear in the history but has to appear in the prediction: it will be used in generation to stop the generator upon reaching the end of the sentence. \n",
    "    \n",
    "Taking all possible (history, prediction) pairs might yield too many training data so you might want to downsize if necessary to somewhere around 500,000 samples. This can be done with a step parameter when you scan an input sequence, e.g., something like\n",
    "\n",
    "```python\n",
    "for i, utterance in enumerate(pos_texts_encoded):    \n",
    "    for j in range(0, len(utterance) - input_length, step):\n",
    "        ### check if utterance[j:j+input_length] is a valid history\n",
    "        ### check of utterance[j+input_length] is a valid prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce6821e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "number of sequences for training = 508437\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_length = 5     # define length of history\n",
    "\n",
    "\n",
    "def create_x_y(encoded_texts, nb_sample_max=None, step=1):\n",
    "    X = []\n",
    "    Y = []\n",
    "    point_indice = vocab['.']\n",
    "\n",
    "    if nb_sample_max is None:\n",
    "        for text in encoded_texts:\n",
    "            for i in range(0, len(text) - input_length, step):\n",
    "                if text[i+input_length] != 0 and text[i+input_length] != len(vocab):\n",
    "                    X.append(text[i:i+input_length])\n",
    "                    Y.append(text[i+input_length])\n",
    "            X.append(text[-input_length:])\n",
    "            Y.append(point_indice)\n",
    "\n",
    "    else:\n",
    "        for text in encoded_texts:\n",
    "            if nb_sample_max <= 0:\n",
    "                break\n",
    "            for i in range(0, len(text) - input_length, step):\n",
    "                if text[i+input_length] != 0 and text[i+input_length] != len(vocab):\n",
    "                    nb_sample_max -= 1\n",
    "                    X.append(text[i:i+input_length])\n",
    "                    Y.append(text[i+input_length])\n",
    "            X.append(text[-input_length:])\n",
    "            Y.append(point_indice)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X_good, Y_good = create_x_y(encoded_pos_texts, 500000, 4)\n",
    "\n",
    "\n",
    "print('number of sequences for training =', len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb826692",
   "metadata": {},
   "source": [
    "### Define and train a RNN LM\n",
    "\n",
    "The following two cells define the model and run training. You should only adapt things here, in particular to your vocabulary size in the first cell. The rest can be left untouched.\n",
    "\n",
    "**TODO**\n",
    "\n",
    "Draw the architecture of the model that is defined and take a look at\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "for the definition of the sparse categorical cross-entropy loss function. How adapted is it to our problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ce63820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/Cellar/python@3.11/3.11.5/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">666,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6664</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">673,064</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │       \u001b[38;5;34m666,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m80,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6664\u001b[0m)           │       \u001b[38;5;34m673,064\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,864</span> (5.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,419,864\u001b[0m (5.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,419,864</span> (5.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,419,864\u001b[0m (5.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Define the model and the hyperparameters such as embedding dimension and LSTM state dimension.\n",
    "#\n",
    "\n",
    "\n",
    "vocab_size = len(vocab) ### set to your vocabulary size, i.e., number of tokens in the vocabulary, including <unk>\n",
    "embedding_size = 100 # dimension of the input embeddings\n",
    "lstm_size = 100 # dimension of the RNN state\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_shape = (input_length,)))\n",
    "model.add(LSTM(lstm_size))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4f60a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3178/3178\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 19ms/step - loss: 5.1394 - val_loss: 5.1154\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# And finally run training with an early stopping criterion\n",
    "#\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "val_split = 0.2\n",
    "\n",
    "stop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 1, mode = 'auto')\n",
    "save = ModelCheckpoint('data.NOSAVE/lstm-10-5.keras', monitor = 'val_loss', verbose = 0, save_best_only = True)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "history = model.fit(X, Y, batch_size = batch_size, epochs = epochs, verbose = 1, validation_split = val_split, callbacks = [stop, save])\n",
    "\n",
    "model.save('bad_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33223f8",
   "metadata": {},
   "source": [
    "### Play with the model\n",
    "\n",
    "Now we have a properly trained model able to take in a few tokens and output a probability distribution function over the vocabulary for the next position (i.e., p[.|h]). Before going for text generation, let's first see on test data how prediction for a given history differs from the reality. The idea is to generate data as for training on new (test) data, take the prediction for a given history along with its probability and see how it differs from the truth in the data (here again, looking at the probabilities). We typically want to have an output like this\n",
    "\n",
    "```\n",
    "['the', 'dead', 'of', 'winter', 'i']                 best = can (0.048)   true = 've (0.014)\n",
    "['winter', 'i', \"'ve\", 'been', 'there']              best = in (0.148)   true = , (0.074)\n",
    "['been', 'there', ',', 'and', 'this']                best = is (0.417)   true = is (0.417)\n",
    "```\n",
    "\n",
    "where best is the predicted token that follows the history with the corresponding probability in the LM, and true is the actual token with the corresponding LM probability.\n",
    "\n",
    "We will first prepare some test data from positive and negative comments. We will then define a predict function that takes as input an history and predicts the next token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66f9190f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#\\n# Get a few unseen texts from the database and process them\\n#\\n[clean_utterance(word_tokenize(x[1])) for x in imdb_data if x[0] == 'pos']\\npos_tests = [clean_utterance(word_tokenize(x[1])) for x in imdb_data[2000:2050]]\\nneg_tests = [clean_utterance(word_tokenize(x[1])) for x in imdb_data[-50:]]\\n\\nX1, Y1 = create_x_y([encode_sequence(x, vocab) for x in pos_tests])\\n        \\nprint('number of positive sequences for testing =', len(X1))\\n\\nX2, Y2 = create_x_y([encode_sequence(x, vocab) for x in neg_tests])\\nprint('number of negative sequences for testing =', len(X2))\\n\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#\n",
    "# Get a few unseen texts from the database and process them\n",
    "#\n",
    "[clean_utterance(word_tokenize(x[1])) for x in imdb_data if x[0] == 'pos']\n",
    "pos_tests = [clean_utterance(word_tokenize(x[1])) for x in imdb_data[2000:2050]]\n",
    "neg_tests = [clean_utterance(word_tokenize(x[1])) for x in imdb_data[-50:]]\n",
    "\n",
    "X1, Y1 = create_x_y([encode_sequence(x, vocab) for x in pos_tests])\n",
    "        \n",
    "print('number of positive sequences for testing =', len(X1))\n",
    "\n",
    "X2, Y2 = create_x_y([encode_sequence(x, vocab) for x in neg_tests])\n",
    "print('number of negative sequences for testing =', len(X2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "84cd162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Complete the predict function below that takes as input a model and a history, runs the history through \n",
    "# the model to get the probability distribution over the vocabulary and returns the predicted token. \n",
    "#\n",
    "# The function returns the three following elements:\n",
    "#   - id of the token predicted as following the history\n",
    "#   - probability for this token\n",
    "#   - probability for the true_i token (or 0 if none is specified -- see below)\n",
    "#\n",
    "# Anticipating text generation, the predict function has two modes:\n",
    "# - best: returns the token with highest probability (to follow the history)\n",
    "# - random: returns a random token chosen according to the probability distribution over the vocabulary\n",
    "#\n",
    "# The function can also take a true_i argument which typically corresponds to the id of the token that \n",
    "# actually follows the history in the data. If given, predict returns the probability for this token to \n",
    "# follow the history. Else returns a probability of 0.\n",
    "\n",
    "def predict(model, h: list[int], mode = 'best', true_i = None):\n",
    "    '''\n",
    "    Return a predicted token given the history and the model. Said more simply, predict p[.|h]\n",
    "    with the model and take the best guess or a random guess (depending on mode).\n",
    "    \n",
    "    Returns predicted token with the corresponding probability, optionnally returning the activation \n",
    "    prob of the true token if true_i is provided\n",
    "    '''\n",
    "    h_array = np.array(h).reshape(1, -1)\n",
    "    \n",
    "    probs = model.predict(h_array, verbose=0)[0]\n",
    "    \n",
    "    if mode == 'best':\n",
    "        pred_token_id = np.argmax(probs)\n",
    "        pred_prob = probs[pred_token_id]\n",
    "\n",
    "    else:\n",
    "        pred_token_id = np.random.choice(len(probs), p=probs)\n",
    "        pred_prob = probs[pred_token_id]\n",
    "    \n",
    "    true_prob = probs[true_i] if true_i is not None else 0\n",
    "    \n",
    "    return pred_token_id, pred_prob, true_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41cc7589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "['the', 'dead', 'of', 'winter', 'i']                 best = have (0.060)   random = 'do (0.030)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "['winter', 'i', ''ve', 'been', 'there']              best = to (0.092)   random = 'with (0.016)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "['been', 'there', ',', 'and', 'this']                best = is (0.158)   random = 'quite (0.000)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Use the predict function above on a few sentence from pos_tests and neg_tests to mimic the following output\n",
    "#\n",
    "# ['the', 'dead', 'of', 'winter', 'i']                 best = can (0.048)   true = 've (0.014)\n",
    "# ['winter', 'i', \"'ve\", 'been', 'there']              best = in (0.148)   true = , (0.074)\n",
    "# ['been', 'there', ',', 'and', 'this']                best = is (0.417)   true = is (0.417)\n",
    "#\n",
    "# Are there significant differences between predictions on the positive samples and that of \n",
    "# the negative samples (assuming a model trained on either positive or negative comments)\n",
    "\n",
    "prediction_best = predict(model, [vocab['the'], vocab['dead'], vocab['of'], vocab['winter'], vocab['i']], 'best')\n",
    "prediction_random = predict(model, [vocab['the'], vocab['dead'], vocab['of'], vocab['winter'], vocab['i']], 'random')\n",
    "print(f\"['the', 'dead', 'of', 'winter', 'i']                 best = {int_to_word[prediction_best[0]]} ({prediction_best[1]:.3f})   random = '{int_to_word[prediction_random[0]]} ({prediction_random[1]:.3f})\")\n",
    "\n",
    "prediction_best = predict(model, [vocab['winter'], vocab['i'], vocab[\"'ve\"], vocab['been'], vocab['there']], 'best')\n",
    "prediction_random = predict(model, [vocab['winter'], vocab['i'], vocab[\"'ve\"], vocab['been'], vocab['there']], 'random')\n",
    "print(f\"['winter', 'i', ''ve', 'been', 'there']              best = {int_to_word[prediction_best[0]]} ({prediction_best[1]:.3f})   random = '{int_to_word[prediction_random[0]]} ({prediction_random[1]:.3f})\")\n",
    "\n",
    "prediction_best = predict(model, [vocab['been'], vocab['there'], vocab[','], vocab['and'], vocab['this']], 'best')\n",
    "prediction_random = predict(model, [vocab['been'], vocab['there'], vocab[','], vocab['and'], vocab['this']], 'random')\n",
    "print(f\"['been', 'there', ',', 'and', 'this']                best = {int_to_word[prediction_best[0]]} ({prediction_best[1]:.3f})   random = '{int_to_word[prediction_random[0]]} ({prediction_random[1]:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e10c2",
   "metadata": {},
   "source": [
    "## Natural language generation\n",
    "\n",
    "Finally, we can move on to text generation, which is sort of straightforward given the predict function above. The idea is to give a prompt and let the generator complete, e.g., \n",
    "\n",
    "```python\n",
    "prompt = encode_sequence(['it', \"'s\", 'hard', 'to', 'believe'])\n",
    "sample = generate(prompt, model)\n",
    "```\n",
    "\n",
    "outputs\n",
    "\n",
    "```\n",
    "it 's hard to believe that the film is not a good movie .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bc834b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Write a function generate that takes as input a prompt (a list of token ids correponding to the\n",
    "# begining of the text with a minimum length of the history size you defined) and iteratively \n",
    "# generate the following tokens. The generation loop stops when a punctuation is generated (.!?) \n",
    "# or when MAX_SENTENCE_SIZE is reached. \n",
    "#\n",
    "# You can do the generation loop selecting either the best following token at each iteration or \n",
    "# simply taking a random one according to the probability distribution function defined by the \n",
    "# current history. You're invited to compare and comment the two.\n",
    "#\n",
    "\n",
    "MAX_SENTENCE_SIZE = 100\n",
    "\n",
    "def generate(prompt, model, mode):\n",
    "    '''\n",
    "    Generate text starting from the prompt.\n",
    "    '''\n",
    "    if mode == 'mix':\n",
    "        mode_mix = True\n",
    "        mode = 'best'\n",
    "    else:\n",
    "        mode_mix = False\n",
    "\n",
    "    prompt_list = prompt.split()\n",
    "    encoded_text = []\n",
    "    text = [clean_utterance(word_tokenize(word)) for word in prompt_list[-5:]]\n",
    "\n",
    "    for word in text:\n",
    "        if word[0] not in vocab:\n",
    "            encoded_text.append(0)\n",
    "        else:\n",
    "            encoded_text.append(vocab[word[0]])\n",
    "\n",
    "\n",
    "    iteration = 0\n",
    "    while iteration < MAX_SENTENCE_SIZE:\n",
    "        prediction = predict(model, encoded_text[-5:], mode)\n",
    "        encoded_text.append(prediction[0])\n",
    "        if prediction[0] == vocab['.']:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if mode_mix:\n",
    "            mode = 'random' if mode == 'best' else 'best'\n",
    "    \n",
    "    # Decode the text\n",
    "    for number in encoded_text:\n",
    "        print(int_to_word[number], end=' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "68bf96ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello elm cable and the ship feature . "
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('good__model_1.keras')\n",
    "\n",
    "# Different modes : 'best', 'random', 'mix'\n",
    "mode = 'random'\n",
    "\n",
    "prompt = \"Hello\"\n",
    "\n",
    "generate(prompt, model, mode)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
