{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561c3744",
   "metadata": {},
   "source": [
    "# Homemade chat GPT (from IMDb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8fa8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 21:19:47.691987: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-15 21:19:47.695412: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-15 21:19:47.741482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-15 21:19:48.854006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home/valentin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the modules\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, download\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f8fa9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions\n",
    "\n",
    "\n",
    "def get_token_counts(data: list) -> dict:\n",
    "    '''\n",
    "    Create vocabulary from a bunch of (tokenized) texts. \n",
    "\n",
    "    Args:\n",
    "        - data (list): list of tokenized texts\n",
    "    \n",
    "    Returns:\n",
    "        - token count (dict)\n",
    "    '''\n",
    "    dict_token = {}    \n",
    "    \n",
    "    for utterance in data:\n",
    "        for token in utterance:\n",
    "            dict_token[token] = 1 if token not in dict_token else dict_token[token] + 1\n",
    "\n",
    "    return dict(sorted(dict_token.items(), key=lambda x: x[1], reverse = True))\n",
    "\n",
    "\n",
    "def encode_sequence(tokens: list[str], vocab: dict) -> list[int]:\n",
    "    '''\n",
    "    Encode a list of tokens into a list of integers using a vocabulary.\n",
    "    '''\n",
    "    \n",
    "    return [vocab[token] if token in vocab else 0 for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88573de",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f748c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the tokens\n",
    "\n",
    "def clean_tokens(buf: list[str]) -> list[str]:\n",
    "    '''\n",
    "    Clean the input list of tokens by removing punctuation marks, special characters,\n",
    "    periods at the end of words, and splitting tokens at '/'.\n",
    "\n",
    "    Args:\n",
    "        - buf (list): list of tokens\n",
    "    \n",
    "    Returns:\n",
    "        - list of cleaned tokens\n",
    "    '''\n",
    "    # Define a set of characters to ignore\n",
    "    ignore = ['br', '']\n",
    "\n",
    "    # One letter tokens accepted\n",
    "    one_letter_tokens = ['a', 'i', '.', ',', '!', '?', ';', ':', '(', ')']\n",
    "    \n",
    "    # Filter out tokens containing only punctuation marks, special characters,\n",
    "    # and remove periods at the end of words\n",
    "    cleaned_tokens = []\n",
    "    for token in buf:\n",
    "        if token in ignore:\n",
    "            continue\n",
    "        elif len(token) == 1:\n",
    "            if token in one_letter_tokens:\n",
    "                cleaned_tokens.append(token)\n",
    "        elif '/' in token:\n",
    "            # Split token at '/'\n",
    "            separated_words = token.split('/')\n",
    "            buf.extend([word for word in separated_words if word != ''])\n",
    "        elif '.' in token:\n",
    "            # Split token at '.'\n",
    "            separated_words = token.split('.')\n",
    "            buf.extend([word for word in separated_words if word != ''])\n",
    "        elif '\\'' in token:\n",
    "            # Split token at \"\\'\"\n",
    "            separated_words = token.split('\\'')\n",
    "            buf.extend([word for word in separated_words if word != ''])\n",
    "        else:\n",
    "            if token.endswith('.') or token.endswith('-') or token.endswith(\"_\") or token.endswith(\"`\"):\n",
    "                token = token[:-1]  # Remove the period at the end\n",
    "            if token.startswith(\".\") or token.startswith('-') or token.startswith(\"_\") or token.startswith(\"`\"):\n",
    "                token = token[1:]\n",
    "            if token != '':\n",
    "                cleaned_tokens.append(token)\n",
    "    \n",
    "    # Convert tokens to lowercase\n",
    "    cleaned_tokens = [token.lower() for token in cleaned_tokens]\n",
    "\n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c308741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tokenization for positive texts\n",
      "Starting tokenization for negative texts\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open('./IMDb_dataset.json', 'rt') as f:\n",
    "    imdb_data = json.load(f)\n",
    "\n",
    "# Create a list with all sentences, separated by positive and negative\n",
    "print(\"Starting tokenization for positive texts\")\n",
    "pos_texts = [clean_tokens(word_tokenize(x[1])) for x in imdb_data if x[0] == 'pos']\n",
    "\n",
    "\n",
    "print(\"Starting tokenization for negative texts\")\n",
    "neg_texts = [clean_tokens(word_tokenize(x[1])) for x in imdb_data if x[0] == 'neg']\n",
    "\n",
    "\n",
    "all_texts = pos_texts + neg_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "123c4718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  7149\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary (we do the same for positive and negative texts to avoid having different vocabularies for each class)\n",
    "\n",
    "# We count the occurrences of each token in the texts\n",
    "dict_occurences = get_token_counts(all_texts)\n",
    "\n",
    "\n",
    "MINOCC = 50 # PARAM : The number of occurence of a word to be considered as in the vocabulary\n",
    "\n",
    "int_to_word = []\n",
    "vocab = {}\n",
    "\n",
    "int_id = 1\n",
    "vocab['<unk>'] = 0\n",
    "int_to_word.append('<unk>')\n",
    "for token in dict_occurences:\n",
    "    if dict_occurences[token] >= MINOCC:\n",
    "        vocab[token] = int_id\n",
    "        int_to_word.append(token)\n",
    "        int_id += 1\n",
    "\n",
    "print(\"Vocabulary size: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd059ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All texts have been tokenized and encoded\n"
     ]
    }
   ],
   "source": [
    "encoded_pos_texts = [encode_sequence(x, vocab) for x in pos_texts]\n",
    "encoded_neg_texts = [encode_sequence(x, vocab) for x in neg_texts]\n",
    "encoded_all_texts = encoded_pos_texts + encoded_neg_texts\n",
    "\n",
    "print(\"All texts have been tokenized and encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce6821e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences for training for good : 508559\n",
      "Number of sequences for training for bad : 508713\n",
      "Number of sequences for training total : 1017272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_length = 5     # Define length of history\n",
    "\n",
    "\n",
    "def create_x_y(encoded_texts, nb_sample_max=None, step=1):\n",
    "    X = []\n",
    "    Y = []\n",
    "    point_indice = vocab['.']\n",
    "\n",
    "    if nb_sample_max is None:\n",
    "        for text in encoded_texts:\n",
    "            for i in range(0, len(text) - input_length, step):\n",
    "                if text[i+input_length] != 0 and text[i+input_length] != len(vocab):\n",
    "                    X.append(text[i:i+input_length])\n",
    "                    Y.append(text[i+input_length])\n",
    "            X.append(text[-input_length:])\n",
    "            Y.append(point_indice)\n",
    "\n",
    "    else:\n",
    "        for text in encoded_texts:\n",
    "            if nb_sample_max <= 0:\n",
    "                break\n",
    "            for i in range(0, len(text) - input_length, step):\n",
    "                if text[i+input_length] != 0 and text[i+input_length] != len(vocab):\n",
    "                    nb_sample_max -= 1\n",
    "                    X.append(text[i:i+input_length])\n",
    "                    Y.append(text[i+input_length])\n",
    "            X.append(text[-input_length:])\n",
    "            Y.append(point_indice)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "X_good, Y_good = create_x_y(encoded_pos_texts, nb_sample_max=500000, step=4)\n",
    "X_bad, Y_bad = create_x_y(encoded_neg_texts, nb_sample_max=500000, step=4)\n",
    "X_total = X_good + X_bad\n",
    "Y_total = Y_good + Y_bad\n",
    "\n",
    "\n",
    "print('Number of sequences for training for good :', len(X_good))\n",
    "print('Number of sequences for training for bad :', len(X_bad))\n",
    "print('Number of sequences for training total :', len(X_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb826692",
   "metadata": {},
   "source": [
    "## Creation of the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce63820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/Cellar/python@3.11/3.11.5/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 100 # dimension of the input embeddings  # TODO : play with this (300)\n",
    "lstm_size = 100 # dimension of the RNN state\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_shape = (input_length,)))\n",
    "model.add(LSTM(lstm_size))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f60a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3180/3180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 22ms/step - loss: 6.2477 - val_loss: 5.5685\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "\n",
    "epochs = 1  # PARAM : The number of time we pass over the data\n",
    "batch_size = 128\n",
    "val_split = 0.2\n",
    "\n",
    "stop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 5, verbose = 1, mode = 'auto')\n",
    "save = ModelCheckpoint('save/saved_model.keras', monitor = 'val_loss', verbose = 0, save_best_only = True)\n",
    "\n",
    "X_bad = np.array(X_bad)\n",
    "Y_bad = np.array(Y_bad)\n",
    "\n",
    "X_good = np.array(X_good)\n",
    "Y_good = np.array(Y_good)\n",
    "\n",
    "X_total = np.array(X_total)\n",
    "Y_total = np.array(Y_total)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_bad, Y_bad, batch_size = batch_size, epochs = epochs, verbose = 1, validation_split = val_split, callbacks = [stop, save])\n",
    "\n",
    "model.save('bad_model_1_epoch.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84cd162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to predict next token\n",
    "\n",
    "def predict(model, h: list[int], mode = 'best', true_i = None):\n",
    "    '''\n",
    "    Return a predicted token given the history and the model. Said more simply, predict p[.|h]\n",
    "    with the model and take the best guess or a random guess (depending on mode).\n",
    "    \n",
    "    Returns predicted token with the corresponding probability, optionnally returning the activation \n",
    "    prob of the true token if true_i is provided\n",
    "    '''\n",
    "    h_array = np.array(h).reshape(1, -1)\n",
    "    \n",
    "    probs = model.predict(h_array, verbose=0)[0]\n",
    "    \n",
    "    if mode == 'best':\n",
    "        pred_token_id = np.argmax(probs)\n",
    "        pred_prob = probs[pred_token_id]\n",
    "\n",
    "    else:\n",
    "        pred_token_id = np.random.choice(len(probs), p=probs)\n",
    "        pred_prob = probs[pred_token_id]\n",
    "    \n",
    "    true_prob = probs[true_i] if true_i is not None else 0\n",
    "    \n",
    "    return pred_token_id, pred_prob, true_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e10c2",
   "metadata": {},
   "source": [
    "## Natural language generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc834b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "\n",
    "MAX_SENTENCE_SIZE = 100  # The maximum number of token generated\n",
    "\n",
    "def generate(prompt, model, mode):\n",
    "    '''\n",
    "    Generate text starting from the prompt.\n",
    "    '''\n",
    "    if mode == 'mix':\n",
    "        mode_mix = True\n",
    "        mode = 'best'\n",
    "    else:\n",
    "        mode_mix = False\n",
    "\n",
    "    prompt_list = prompt.split()\n",
    "    encoded_text = []\n",
    "    text = [clean_tokens(word_tokenize(word)) for word in prompt_list[-5:]]\n",
    "\n",
    "    for word in text:\n",
    "        if word[0] not in vocab:\n",
    "            encoded_text.append(0)\n",
    "        else:\n",
    "            encoded_text.append(vocab[word[0]])\n",
    "\n",
    "\n",
    "    iteration = 0\n",
    "    while iteration < MAX_SENTENCE_SIZE:\n",
    "        prediction = predict(model, encoded_text[-5:], mode)\n",
    "        encoded_text.append(prediction[0])\n",
    "        if prediction[0] == vocab['.']:\n",
    "            break\n",
    "        iteration += 1\n",
    "        if mode_mix:\n",
    "            mode = 'random' if mode == 'best' else 'best'\n",
    "    \n",
    "    # Decode the text\n",
    "    for number in encoded_text:\n",
    "        print(int_to_word[number], end=' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68bf96ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 27 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7216cc6da520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "hello of the movie . "
     ]
    }
   ],
   "source": [
    "# It's here we can finally play\n",
    "\n",
    "model = tf.keras.models.load_model('bad_model_1_epoch.keras')\n",
    "\n",
    "# Different modes : 'best', 'random', 'mix'\n",
    "mode = 'best'\n",
    "\n",
    "# The prompt you want to start with\n",
    "prompt = \"Hello\"\n",
    "\n",
    "generate(prompt, model, mode)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
